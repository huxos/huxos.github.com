<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
    <title>Huxos Blog</title>
    
    <meta name="description" content="Huxos Blog">
    
    <meta name="author" content="Huxos">
    <link rel="icon" href="/favicon.ico" type="image/x-icon">
    <link rel="shortcut icon" href="/favicon.ico" type="image/x-icon">
    <link rel="author" href="//huxos.me">
    <link href="http://localhost:4000/blog/" rel="canonical">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
    <link rel="alternate" type="application/rss+xml" title="Huxos" href="http://localhost:4000/feed.xml">
    <link rel="stylesheet" href="//cdn.staticfile.org/fluidbox/1.3.1/jquery.fluidbox.css">
    <link type="text/css" rel="stylesheet" href="/assets/application.css">
    <script src="//libs.baidu.com/jquery/1.11.1/jquery.min.js"></script>
    <script src="//cdn.staticfile.org/fluidbox/1.3.1/jquery.fluidbox.min.js"></script>
    <script async="" src="//www.google-analytics.com/analytics.js"></script>
  </head>

  <body>
  <header class="site-header" role="banner">
      <div class="container">
        <a href="/" class="logo"><img src="/assets/logo.png" alt="Huxos" class="site-logo"></a>
        <nav role="navigiation">
          <ul class="site-nav">
            <li><a href="/about/">About</a>
            <li><a href="/blog/">Blog</a>
          </ul>
        </nav>
      </div>
    </header>

    

    <div class="site-content">
      <div class="container">
        <ul class="posts-archive">
  
    <li>
    <a href="/bind/2017/09/23/dns-cluster.html"><h3>基于mysql存储的DNS（bind）部署方案和步骤</h3></a>
      <span class="posts-archive__date">23 Sep 2017</span>
      bind部署的架构 其中master节点采用dlz的方式与使用mysql作为数据后端，slave节点通过zone transfer与master节点同步。 Master不作为服务节点，Slave节点使用LVS作为负载均衡提供服务，这样可以很方便的通过对数据库的CURD操作进行dns记录的变更，同时也可以保留bind原生的性能。 由于master节点的数据保存在mysql中，当数据发生变化之后无法及时感知，slave节点的数据可能会出现与mster不一致的情况。这时我们需要对salve发送notify指令来主动同步master的数据。 可以使用https://github.com/huxos/dns-notify 来发送notify指令。 部署步骤 PS：部署操作系统为：Centos 7, Bind版本为9.11.0-P5。 1．编译bind支持mysql 安装编译依赖 yum install openssl-devel mariadb-devel gcc ln -sf /usr/lib64/mysql/libmysqlclient.so.18 /lib64/libmysqlclient.so 编译bind...
    </li>
  
    <li>
    <a href="/kubernetes/harbor/2017/09/19/kubernetes-cluster-09-harbor.html"><h3>kubernetes集群搭建（9. harbor）</h3></a>
      <span class="posts-archive__date">19 Sep 2017</span>
      kubernetes集群搭建（9. 镜像仓库harbor） Harbor是VMware开发的企业级docker镜像仓库，官方提供了基于docker-compose的方式部署。 花了好些时间将其conpose的编排模版转换成k8s的调度模版方便部署到k8s集群中，项目地址为: https://github.com/huxos/harbor-kubernetes。 部署架构： 把harbor的mysql组件部署到单独的一个Pod中，采用ceph的rbd作为数据库的存储。 新增redis做session共享。 采用ceph的rgw作为镜像的后端存储。 其余组件（包括adminserver、jobservice、ui、nginx、registry）部署到同一个Pod中，使用127.0.0.1通讯。 这样部署可以通过k8s使得harbor灵活的缩放。 部署步骤 部署ceph-rgw并添加访问ceph的用户 由于我们使用ceph的rgw作为registry的后端，首先参照之前部署ceph的步骤kubernetes集群搭建（2. Ceph） 在kube-system-2 kube-system-4上面部署ceph-rgw ceph-deploy rgw create kube-system-2 kube-system-4 添加访问rgw的账户：...
    </li>
  
    <li>
    <a href="/kubernetes/prometheus/2017/09/19/kubernetes-cluster-08-prometheus.html"><h3>kubernetes集群搭建（8. prometheus）</h3></a>
      <span class="posts-archive__date">19 Sep 2017</span>
      Prometheus 的设计非常适合k8s集群的监控。 大多数k8s的组件都提供prometheus格式的监控接口，只需要配置好kube-api作为promethues的sd就能非常容易的监控整个k8s集群，无需引入额外的依赖。 prometheus提供了众多语言的库，可以非常容易的嵌入到业务代码中做业务监控。 可以采用CoreOS所提供的 prometheus-operator来部署prometheus， 但是使用过程中发现其有些较不灵活的地方。比如：只能使用promethues-operator预定义的参数启动prometheus。 现将prometheus-operator生成的规则导出来做成configmap，独立部署prometheus，同时加入相关的规则使得其能够自动根据service的annotation进行发现和监控。 部署步骤 使用statefulset的方式部署prometheus，监控数据存储在ceph的rbd里面。 考虑到1.7版本的prometheus在数据轮换的时候产生大量的IO开销，所以部署了prometheus2.0 beta。 一些准备工作 首先创建monitoring这个namespaces # kubectl create ns monitoring 参照前面的kubernetes集群搭建（2. Ceph）， 在monitoring这个namespace中要使用RBD先要作为secret导入ceph的key。 kubectl...
    </li>
  
    <li>
    <a href="/kubernetes/2017/09/19/kubernetes-cluster-07-ingress.html"><h3>kubernetes集群搭建（7. ingress）</h3></a>
      <span class="posts-archive__date">19 Sep 2017</span>
      An Ingress is a collection of rules that allow inbound connections to reach the cluster services. Ingress的引入主要解决创建入口站点规则的问题，主要作用于7层入口(http)。 可以通过K8s的Ingress对象定义类似于nginx中的vhost、localtion、upstream等。 Nginx官方也有Ingress的实现nginxinc/kubernetes-ingress来对接k8s。 traefik Træfik (pronounced like...
    </li>
  
    <li>
    <a href="/kubernetes/2017/09/19/kubernetes-cluster-06-loadbancer.html"><h3>kubernetes集群搭建（6. loadbancer）</h3></a>
      <span class="posts-archive__date">19 Sep 2017</span>
      kubernetes在定义Service的时候提供了LoadBalancer类型，用于集群外部访问到kubernetes的服务。 但是这里的LoadBalancer主要对接用云服务商提供的负载均衡服务，在物理机部署的环境就不是很适用了。 结合kubernetes本身的Service就提供了负载均衡的功能，想到一个巧妙的方法: 把Service的Cluster IP做成可以在集群外部可以路由。 可以在交换机上面配置等价路由，将Cluster IP段路由到kube-proxy的节点就行了，这种方式最多支持8个节点做集群的负载均衡。 考虑到kube-proxy的冗长的iptables规则，当service的数目多了之后性能上会存在问题，所以采用kube-router作为负载均衡节点。 kube-router基于IPVS实现，转发效率上会更高，规则可读性也会更好。 如果集群网络是采用overlay，那么这种方式的可能的瓶颈在与做负载均衡的节点对集群内外的流量解封包的过程，小规模使用起来应该是没有问题。 另外即将发布的kubernetes 1.8中kube-proxy原生就支持IPVS，到时可以少引入kube-router这依赖了。 kube-router A distributed load balancer, firewall and router designed for Kubernetes。...
    </li>
  
    <li>
    <a href="/kubernetes/2017/09/19/kubernetes-cluster-05-kube-addons.html"><h3>kubernetes集群搭建（5. kube-dns、dashboard）</h3></a>
      <span class="posts-archive__date">19 Sep 2017</span>
      kube-dns、dashboard以deployment的方式部署到kubernetes，运行在kube-system这个namespace中。 只需要导入相应的模版就行，部署较为简单。 下载 # wget https://github.com/kubernetes/kubernetes/releases/download/v1.7.4/kubernetes.tar.gz -O - |tar -zxpvf - # cd kubernetes kube-dns ~/kubernetes # cd cluster/addons/dns ~/k/c/a/dns # rm -rf...
    </li>
  
    <li>
    <a href="/kubernetes/2017/09/19/kubernetes-cluster-04-kube-node.html"><h3>kubernetes集群搭建（4. kube node）</h3></a>
      <span class="posts-archive__date">19 Sep 2017</span>
      本例Kube Node节点的kubelet通过VIP 10.22.108.250与APIServer通讯。kubelet第一次通过启动tls bootstrap认证后，由apiserver生成节点的证书。 部署步骤 网络组建部署 参照上篇kubernetes集群搭建（3. Kube master） 配置好flanneld 以及CNI 部署kubelet 生成kubectl获取认证的bootstrap-kubeconfig 使用如下命令生成bootstrap-kubeconfig.yaml kubectl config set-cluster kubernetes \ --certificate-authority=cert/ca.pem \ --embed-certs=true \...
    </li>
  
    <li>
    <a href="/kubernetes/2017/09/19/kubernetes-cluster-03-kube-master.html"><h3>kubernetes集群搭建（3. Kube master）</h3></a>
      <span class="posts-archive__date">19 Sep 2017</span>
      本例中部署了kubernetes部署了Master三个节点，可以使用部署Keepalived做Apiserver的高可用。 其中kubelet是采用systemd启动在物理机上面的，其他组件通过kubernetes的manifests启动在容器里面。 三个Kubernetes Master的IP地址为：10.22.108.20、10.22.108.79、10.22.108.92，假定使用10.22.108.250作为Apiserver的VIP。 k8s集群配置 kube-apiserver: 10.22.108.20,10.22.108.79,10.22.108.92, VIP: 10.22.108.250 service-cidr: 10.99.66.0/23 cluster-domain: cluster.local cluster-cidr: 10.20.0.0/16 apiserver-service-ip: 10.99.66.1 kubedns-service-ip: 10.99.66.2 部署过程 生成证书 采用CloudFlare的PKI工具集 cfssl来制作证书。...
    </li>
  
    <li>
    <a href="/kubernetes/ceph/2017/09/19/kubernetes-cluster-02-ceph.html"><h3>kubernetes集群搭建（2. Ceph）</h3></a>
      <span class="posts-archive__date">19 Sep 2017</span>
      本例中使用5个机器部署ceph/dev/sdb作为osd使用的分区，使用ceph-deploy搭建了简易的ceph集群。 该Ceph集群用来存储包括docker镜像仓库，prometheus的监控数据等。 使用kubernetes的StorageClass对接ceph，使得ceph的rdb可以作为数据卷挂载给Pod使用。 安装ceph-deploy apt-get install python-pip virtualenv virtualenv ~/.env source ~/.env/bin/activate pip install ceph-deploy --index-url https://pypi.tuna.tsinghua.edu.cn/simple 使用ceph-deploy安装ceph 注意： 安装ceph要求mon节点时间要同步，可以先配置好ntpd做时间同步。采用命令timedatectl status或者ntpq -p查看同步状态。 首先编辑5台机器的hosts文件加入如下内容...
    </li>
  
    <li>
    <a href="/kubernetes/docker/2017/09/19/kubernetes-cluster-01-docker.html"><h3>kubernetes集群搭建（1. docker）</h3></a>
      <span class="posts-archive__date">19 Sep 2017</span>
      操作系统 系统选择：Ubuntu 16.04.2 LTS, 内核版本4.10.0-27-generic 安装操作系统： 尽量保持/var/是最大的分区，docker和kubernete 的数据默认保存在此分区，采用xfs文件系统。 如果/var/分区较小需要改变docker启动参数指定数据所在目录，例如：-g /home/docker 操作系统调优：做好操作系统基本的调优比如内核参数调优，设置ulimit，网卡软中断绑定等。 安装docker 参照Docker官方提供的方法安装docker-ce最新版本。 sudo apt-get install \ apt-transport-https \ ca-certificates \ curl \...
    </li>
  
</ul>
<!-- 分页链接 -->
<div class="pagination">
  
    <span class="previous">Previous</span>
  
  <span class="page_number">Page: 1 of 2</span>
  
    <a href="/blog/page2" class="next">Next</a>
  
</div>

      </div>
    </div>

    <footer class="site-footer">
      <div class="container">
        © Huxos 2014.
        <br>
        liberty, when it begins to take root, it a plant of rapid growth.
        <br>
        Powered By Jekyll on Github, Forked from <a href="https://github.com/RichGuk/richguk.github.io">RichGuk</a>.
      </div>
    </footer>
    <script>
      (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
        (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
          m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
      })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
      ga('create', 'UA-36713694-1', 'huxos.me');
      ga('send', 'pageview');
      $(function() {
        $('a[data-fluidbox]').fluidbox();
      });
    </script>
  </body>
</html>
